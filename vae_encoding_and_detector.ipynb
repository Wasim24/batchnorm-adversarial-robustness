{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1088
    },
    "colab_type": "code",
    "id": "I_nXPE3kM5q4",
    "outputId": "c88ab9b6-3014-4a25-9d28-94e939e18c92",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-gpu==2.0.0-alpha0 in /usr/local/lib/python3.6/dist-packages (2.0.0a0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
      "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0a20190301)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.8.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.16.4)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.12.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.15.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.1.7)\n",
      "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.14.0.dev2019030115)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.7)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.9)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.7.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.33.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.15.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.1.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (41.0.1)\n",
      "Uninstalling tfp-nightly-0.8.0.dev20190602:\n",
      "  Would remove:\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow_probability/*\n",
      "    /usr/local/lib/python3.6/dist-packages/tfp_nightly-0.8.0.dev20190602.dist-info/*\n",
      "  Would not remove (might be manually added):\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/bijectors/conditional_bijector.py\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/distributions/conditional_distribution.py\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/distributions/conditional_transformed_distribution.py\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/distributions/internal/statistical_testing.py\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/distributions/sample_stats.py\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/mcmc/slice_sampler_utils.py\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/mcmc/util.py\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/util/variables.py\n",
      "Proceed (y/n)? y\n",
      "  Successfully uninstalled tfp-nightly-0.8.0.dev20190602\n",
      "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability) (1.16.4)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability) (1.12.0)\n",
      "Requirement already up-to-date: tf-nightly-2.0-preview in /usr/local/lib/python3.6/dist-packages (2.0.0.dev20190602)\n",
      "Collecting tfp-nightly\n",
      "  Using cached https://files.pythonhosted.org/packages/5b/0b/126b9d1309d8e219411b67e7dbaee3a3bab4d507186be980ad7ceb2ae8a0/tfp_nightly-0.8.0.dev20190602-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.8.0)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.7.1)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator-2.0-preview in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.14.0.dev2019060200)\n",
      "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.11.1)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.9)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.1.7)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (3.7.1)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.7)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: tb-nightly<1.15.0a0,>=1.14.0a0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.14.0a20190301)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.33.4)\n",
      "Requirement already satisfied, skipping upgrade: cloudpickle>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from tfp-nightly) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.6/dist-packages (from tfp-nightly) (4.4.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-2.0-preview) (41.0.1)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf-nightly-2.0-preview) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-2.0-preview) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-2.0-preview) (0.15.4)\n",
      "Installing collected packages: tfp-nightly\n",
      "Successfully installed tfp-nightly-0.8.0.dev20190602\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "!pip install tensorflow-gpu==2.0.0-alpha0\n",
    "!pip uninstall tfp-nightly\n",
    "!pip install tensorflow-probability\n",
    "!pip install --upgrade tf-nightly-2.0-preview tfp-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kBTbqnIxNb0i",
    "outputId": "41d75442-3120-4e20-f488-cbaa3cb86452"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-dev20190602'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QW1STF5bM5rU"
   },
   "outputs": [],
   "source": [
    "(X_clean_train, Y_clean_train), (X_clean_test, Y_clean_test) = tf.keras.datasets.cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7G5sHa50M5rw"
   },
   "outputs": [],
   "source": [
    "\n",
    "input_shape = (32,32,3)\n",
    "encoded_size = 16\n",
    "base_depth = 32\n",
    "L2_weight_decay = 2e-5\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gzuwlS8EM5r0"
   },
   "outputs": [],
   "source": [
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "initializer = tf.initializers.VarianceScaling(scale=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bo8XVgZmM5r_"
   },
   "outputs": [],
   "source": [
    "def SampleFromEncoding(mean, sigma):\n",
    "    t_sigma = tf.sqrt(tf.exp(sigma))\n",
    "    epsilon = tf.keras.backend.random_normal(shape=tf.shape(mean), mean=0., stddev=1., dtype = tf.float64)\n",
    "    return mean + t_sigma * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UzHlcZzYM5sW"
   },
   "outputs": [],
   "source": [
    "class VaeConvNet(tf.keras.Model):\n",
    "    def __init__(self, input_shape = (32,32,3)):\n",
    "        super(VaeConvNet, self).__init__()  \n",
    "        self.e_conv1 = tf.keras.layers.Conv2D(filters=3 , kernel_size=2, strides=1, activation=tf.nn.relu, padding='same', name=\"e-conv1\", trainable=True)\n",
    "        self.e_conv2 = tf.keras.layers.Conv2D(filters=32, kernel_size=2, strides=2, activation=tf.nn.relu, padding='same', name=\"e-conv2\", trainable=True)\n",
    "        self.e_conv3 = tf.keras.layers.Conv2D(filters=32, kernel_size=2, strides=1, activation=tf.nn.relu, padding='same', name=\"e-conv3\", trainable=True)\n",
    "        self.e_conv4 = tf.keras.layers.Conv2D(filters=32, kernel_size=2, strides=1, activation=tf.nn.relu, padding='same', name=\"e-conv4\", trainable=True)\n",
    "        self.e_dense  = tf.keras.layers.Dense(128, tf.nn.relu, name=\"e-sdnse\", trainable=True)\n",
    "        self.e_mean  = tf.keras.layers.Dense(encoded_size, tf.nn.softplus, name=\"e-mean\", trainable=True)\n",
    "        self.e_sigma  = tf.keras.layers.Dense(encoded_size,tf.nn.softplus, name=\"e-sigma\", trainable=True)\n",
    "        \n",
    "        self.d_dense = tf.keras.layers.Dense(128, name=\"d-dense\", trainable=True)\n",
    "        self.d_dense2 = tf.keras.layers.Dense(32*input_shape[0]*input_shape[1]/4, name=\"d-dense2\", trainable=True)\n",
    "        self.d_convT1 = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=2, strides=1, activation=tf.nn.relu, padding='same', name=\"d-convT1\", trainable=True)\n",
    "        self.d_convT2 = tf.keras.layers.Conv2DTranspose(filters=32 , kernel_size=2, strides=1, activation=tf.nn.relu, padding='same', name=\"d-convT2\", trainable=True)\n",
    "        self.d_convT3 = tf.keras.layers.Conv2DTranspose(filters=3  , kernel_size=2, strides=2, activation=tf.nn.relu, padding='valid', name=\"d-convT3\", trainable=True)\n",
    "        #self.d_conv = tf.keras.layers.Conv2D(filters=3  , kernel_size=2, strides=1, activation=tf.nn.sigmoid, padding='same', name=\"d-conv\", trainable=True)\n",
    "        \n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        x = self.e_conv1(x)\n",
    "        x = self.e_conv2(x)\n",
    "        x = self.e_conv3(x)\n",
    "        x = self.e_conv4(x)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x = self.e_dense(x)\n",
    "        mean = self.e_mean(x)\n",
    "        sigma = self.e_sigma(x)\n",
    "        sample = SampleFromEncoding( mean, sigma)\n",
    "        y = self.d_dense(sample)\n",
    "        y = self.d_dense2(y)\n",
    "        y = tf.keras.layers.Reshape(target_shape=(16,16,32), input_shape=(None, encoded_size))(y)\n",
    "        y = self.d_convT1(y)\n",
    "        y = self.d_convT2(y)\n",
    "        y = self.d_convT3(y)\n",
    "        #y = self.d_conv(y)\n",
    "        return y, mean, sigma    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dFTkVrqkM5sm"
   },
   "outputs": [],
   "source": [
    "def VaeLoss(x, x_reco, mean, sigma):\n",
    "  reconstruction_term = -tf.reduce_sum(tfp.distributions.MultivariateNormalDiag(\n",
    "      tf.keras.layers.Flatten()(x_reco), scale_identity_multiplier=0.05).log_prob(tf.keras.layers.Flatten()(x)))\n",
    "  kl_divergence = 0.5 * tf.reduce_mean(tf.square(mean) + tf.square(sigma) - tf.math.log(1e-8 + tf.square(sigma)) - 1, [1])\n",
    "  return tf.reduce_mean(reconstruction_term + kl_divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rGY5NE-uM5sr"
   },
   "outputs": [],
   "source": [
    "def optimizer_init_fn():\n",
    "    learning_rate = 0.001\n",
    "    return tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "optimizer = optimizer_init_fn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yxug4DWP64Du"
   },
   "outputs": [],
   "source": [
    "model     = VaeConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tOGQcOT1M5s8"
   },
   "outputs": [],
   "source": [
    "def Train(model, optimizer, x, num_epochs, batch_size, is_training=False):\n",
    "    t = 0\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    \n",
    "    num_batches = tf.shape(x)[0]//batch_size\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        train_loss.reset_states()\n",
    "        \n",
    "        for batch in range(num_batches):\n",
    "            x_batch_input = x[batch*batch_size : (batch+1)*batch_size]/255\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                # Use the model function to build the forward pass.\n",
    "                y_output, mean, sigma  = model(x_batch_input, training=True)\n",
    "                loss = VaeLoss(x_batch_input, y_output, mean, sigma )\n",
    "                \n",
    "                if is_training:\n",
    "                  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                \n",
    "            # Update the metrics\n",
    "            train_loss.update_state(loss)\n",
    "\n",
    "            if t % 10 == 0:\n",
    "              template = 'Iteration {}, Epoch {}, Loss: {}'\n",
    "              print (template.format(t, epoch+1, train_loss.result()) )                   \n",
    "            t += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "colab_type": "code",
    "id": "ESVw9lx2M5tA",
    "outputId": "8bd55c5d-9a41-4419-be27-24b09d4a38ce"
   },
   "outputs": [],
   "source": [
    "Train(model, optimizer=optimizer, x=X_clean_train, num_epochs=25, batch_size = batch_size, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "bXTzYIWo_4mD",
    "outputId": "798a66e4-0783-4bfd-9783-727d44315b0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 5029492.0\n",
      "Iteration 10, Epoch 1, Loss: 4722672.5\n",
      "Iteration 20, Epoch 1, Loss: 4697304.5\n",
      "Iteration 30, Epoch 1, Loss: 4686607.0\n",
      "Iteration 40, Epoch 1, Loss: 4653678.0\n",
      "Iteration 50, Epoch 2, Loss: 4880903.5\n",
      "Iteration 60, Epoch 2, Loss: 4583726.0\n",
      "Iteration 70, Epoch 2, Loss: 4603425.0\n",
      "Iteration 80, Epoch 2, Loss: 4616446.0\n",
      "Iteration 90, Epoch 2, Loss: 4597495.5\n",
      "Iteration 100, Epoch 3, Loss: 4873375.0\n",
      "Iteration 110, Epoch 3, Loss: 4574323.0\n",
      "Iteration 120, Epoch 3, Loss: 4594009.5\n",
      "Iteration 130, Epoch 3, Loss: 4606757.5\n",
      "Iteration 140, Epoch 3, Loss: 4587775.0\n",
      "Iteration 150, Epoch 4, Loss: 4864523.5\n",
      "Iteration 160, Epoch 4, Loss: 4563099.0\n",
      "Iteration 170, Epoch 4, Loss: 4583357.5\n",
      "Iteration 180, Epoch 4, Loss: 4596370.0\n",
      "Iteration 190, Epoch 4, Loss: 4577820.5\n",
      "Iteration 200, Epoch 5, Loss: 4854783.0\n",
      "Iteration 210, Epoch 5, Loss: 4553606.0\n",
      "Iteration 220, Epoch 5, Loss: 4573682.5\n",
      "Iteration 230, Epoch 5, Loss: 4586367.0\n",
      "Iteration 240, Epoch 5, Loss: 4567557.5\n",
      "Iteration 250, Epoch 6, Loss: 4845719.5\n",
      "Iteration 260, Epoch 6, Loss: 4543062.5\n",
      "Iteration 270, Epoch 6, Loss: 4563452.0\n",
      "Iteration 280, Epoch 6, Loss: 4576103.0\n",
      "Iteration 290, Epoch 6, Loss: 4557012.5\n",
      "Iteration 300, Epoch 7, Loss: 4835905.5\n",
      "Iteration 310, Epoch 7, Loss: 4534116.5\n",
      "Iteration 320, Epoch 7, Loss: 4554633.0\n",
      "Iteration 330, Epoch 7, Loss: 4567306.5\n",
      "Iteration 340, Epoch 7, Loss: 4548858.0\n",
      "Iteration 350, Epoch 8, Loss: 4826849.5\n",
      "Iteration 360, Epoch 8, Loss: 4524528.5\n",
      "Iteration 370, Epoch 8, Loss: 4545255.0\n",
      "Iteration 380, Epoch 8, Loss: 4558553.5\n",
      "Iteration 390, Epoch 8, Loss: 4539613.0\n",
      "Iteration 400, Epoch 9, Loss: 4817155.0\n",
      "Iteration 410, Epoch 9, Loss: 4514622.0\n",
      "Iteration 420, Epoch 9, Loss: 4535223.5\n",
      "Iteration 430, Epoch 9, Loss: 4548402.0\n",
      "Iteration 440, Epoch 9, Loss: 4529824.5\n",
      "Iteration 450, Epoch 10, Loss: 4809286.5\n",
      "Iteration 460, Epoch 10, Loss: 4505363.0\n",
      "Iteration 470, Epoch 10, Loss: 4525853.0\n",
      "Iteration 480, Epoch 10, Loss: 4538994.5\n",
      "Iteration 490, Epoch 10, Loss: 4520738.5\n"
     ]
    }
   ],
   "source": [
    "Train(model, optimizer=optimizer, x=X_clean_train, num_epochs=10, batch_size = batch_size, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "0Dq3i6g4BY9Q",
    "outputId": "483e241c-416a-4f5b-c796-2748bf6ffce3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 4795487.5\n",
      "Iteration 10, Epoch 1, Loss: 4516942.0\n",
      "Iteration 20, Epoch 1, Loss: 4525826.5\n",
      "Iteration 30, Epoch 1, Loss: 4532755.0\n",
      "Iteration 40, Epoch 1, Loss: 4511228.0\n",
      "Iteration 50, Epoch 2, Loss: 4771861.0\n",
      "Iteration 60, Epoch 2, Loss: 4475030.0\n",
      "Iteration 70, Epoch 2, Loss: 4496811.0\n",
      "Iteration 80, Epoch 2, Loss: 4510669.5\n",
      "Iteration 90, Epoch 2, Loss: 4493101.0\n",
      "Iteration 100, Epoch 3, Loss: 4768635.5\n",
      "Iteration 110, Epoch 3, Loss: 4471935.5\n",
      "Iteration 120, Epoch 3, Loss: 4493577.5\n",
      "Iteration 130, Epoch 3, Loss: 4507244.0\n",
      "Iteration 140, Epoch 3, Loss: 4489442.5\n",
      "Iteration 150, Epoch 4, Loss: 4765326.5\n",
      "Iteration 160, Epoch 4, Loss: 4467712.0\n",
      "Iteration 170, Epoch 4, Loss: 4489090.5\n",
      "Iteration 180, Epoch 4, Loss: 4502918.0\n",
      "Iteration 190, Epoch 4, Loss: 4485296.5\n",
      "Iteration 200, Epoch 5, Loss: 4762504.5\n",
      "Iteration 210, Epoch 5, Loss: 4463289.5\n",
      "Iteration 220, Epoch 5, Loss: 4485148.0\n",
      "Iteration 230, Epoch 5, Loss: 4499182.5\n",
      "Iteration 240, Epoch 5, Loss: 4481353.0\n",
      "Iteration 250, Epoch 6, Loss: 4760703.0\n",
      "Iteration 260, Epoch 6, Loss: 4459586.0\n",
      "Iteration 270, Epoch 6, Loss: 4481394.5\n",
      "Iteration 280, Epoch 6, Loss: 4495015.0\n",
      "Iteration 290, Epoch 6, Loss: 4477042.5\n",
      "Iteration 300, Epoch 7, Loss: 4756482.0\n",
      "Iteration 310, Epoch 7, Loss: 4454562.0\n",
      "Iteration 320, Epoch 7, Loss: 4476345.0\n",
      "Iteration 330, Epoch 7, Loss: 4490021.0\n",
      "Iteration 340, Epoch 7, Loss: 4472383.5\n",
      "Iteration 350, Epoch 8, Loss: 4750200.5\n",
      "Iteration 360, Epoch 8, Loss: 4451003.5\n",
      "Iteration 370, Epoch 8, Loss: 4472324.0\n",
      "Iteration 380, Epoch 8, Loss: 4486188.5\n",
      "Iteration 390, Epoch 8, Loss: 4468532.0\n",
      "Iteration 400, Epoch 9, Loss: 4749000.5\n",
      "Iteration 410, Epoch 9, Loss: 4446986.5\n",
      "Iteration 420, Epoch 9, Loss: 4468365.0\n",
      "Iteration 430, Epoch 9, Loss: 4482448.0\n",
      "Iteration 440, Epoch 9, Loss: 4464576.5\n",
      "Iteration 450, Epoch 10, Loss: 4743346.5\n",
      "Iteration 460, Epoch 10, Loss: 4442492.0\n",
      "Iteration 470, Epoch 10, Loss: 4463273.0\n",
      "Iteration 480, Epoch 10, Loss: 4476924.5\n",
      "Iteration 490, Epoch 10, Loss: 4459463.5\n"
     ]
    }
   ],
   "source": [
    "Train(model, optimizer=optimizer, x=X_clean_train, num_epochs=10, batch_size = batch_size, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "Ov-rHDuo6fHw",
    "outputId": "95a254c0-4c5c-4cc8-cb4e-6ff2e7fc6a48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 4738730.0\n",
      "Iteration 10, Epoch 1, Loss: 4432672.5\n",
      "Iteration 20, Epoch 1, Loss: 4451177.5\n",
      "Iteration 30, Epoch 1, Loss: 4462854.5\n",
      "Iteration 40, Epoch 1, Loss: 4443687.0\n",
      "Iteration 50, Epoch 2, Loss: 4718017.5\n",
      "Iteration 60, Epoch 2, Loss: 4421271.5\n",
      "Iteration 70, Epoch 2, Loss: 4442033.0\n",
      "Iteration 80, Epoch 2, Loss: 4455782.5\n",
      "Iteration 90, Epoch 2, Loss: 4438396.5\n",
      "Iteration 100, Epoch 3, Loss: 4719292.5\n",
      "Iteration 110, Epoch 3, Loss: 4419661.5\n",
      "Iteration 120, Epoch 3, Loss: 4440582.0\n",
      "Iteration 130, Epoch 3, Loss: 4454466.0\n",
      "Iteration 140, Epoch 3, Loss: 4436898.5\n",
      "Iteration 150, Epoch 4, Loss: 4716809.0\n",
      "Iteration 160, Epoch 4, Loss: 4417225.5\n",
      "Iteration 170, Epoch 4, Loss: 4438447.0\n",
      "Iteration 180, Epoch 4, Loss: 4452729.0\n",
      "Iteration 190, Epoch 4, Loss: 4435500.0\n",
      "Iteration 200, Epoch 5, Loss: 4714538.5\n",
      "Iteration 210, Epoch 5, Loss: 4414987.5\n",
      "Iteration 220, Epoch 5, Loss: 4436769.5\n",
      "Iteration 230, Epoch 5, Loss: 4451037.0\n",
      "Iteration 240, Epoch 5, Loss: 4434036.5\n",
      "Iteration 250, Epoch 6, Loss: 4714663.0\n",
      "Iteration 260, Epoch 6, Loss: 4414280.0\n",
      "Iteration 270, Epoch 6, Loss: 4435581.0\n",
      "Iteration 280, Epoch 6, Loss: 4450071.5\n",
      "Iteration 290, Epoch 6, Loss: 4432965.5\n",
      "Iteration 300, Epoch 7, Loss: 4712904.0\n",
      "Iteration 310, Epoch 7, Loss: 4413100.5\n",
      "Iteration 320, Epoch 7, Loss: 4434499.5\n",
      "Iteration 330, Epoch 7, Loss: 4449012.0\n",
      "Iteration 340, Epoch 7, Loss: 4431804.5\n",
      "Iteration 350, Epoch 8, Loss: 4712543.5\n",
      "Iteration 360, Epoch 8, Loss: 4412450.0\n",
      "Iteration 370, Epoch 8, Loss: 4433617.0\n",
      "Iteration 380, Epoch 8, Loss: 4447828.0\n",
      "Iteration 390, Epoch 8, Loss: 4430418.5\n",
      "Iteration 400, Epoch 9, Loss: 4709683.0\n",
      "Iteration 410, Epoch 9, Loss: 4410214.5\n",
      "Iteration 420, Epoch 9, Loss: 4431517.5\n",
      "Iteration 430, Epoch 9, Loss: 4445970.5\n",
      "Iteration 440, Epoch 9, Loss: 4429066.5\n",
      "Iteration 450, Epoch 10, Loss: 4711240.5\n",
      "Iteration 460, Epoch 10, Loss: 4409556.5\n",
      "Iteration 470, Epoch 10, Loss: 4430818.5\n",
      "Iteration 480, Epoch 10, Loss: 4445263.0\n",
      "Iteration 490, Epoch 10, Loss: 4427995.0\n"
     ]
    }
   ],
   "source": [
    "Train(model, optimizer=optimizer, x=X_clean_train, num_epochs=10, batch_size = batch_size, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "ZWrrfgHFVyw4",
    "outputId": "7a8baf3d-476b-4449-e0e9-fd2becb68ef9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 4706355.0\n",
      "Iteration 10, Epoch 1, Loss: 4408084.5\n",
      "Iteration 20, Epoch 1, Loss: 4429587.0\n",
      "Iteration 30, Epoch 1, Loss: 4444014.5\n",
      "Iteration 40, Epoch 1, Loss: 4426845.0\n",
      "Iteration 50, Epoch 2, Loss: 4706973.0\n",
      "Iteration 60, Epoch 2, Loss: 4407222.5\n",
      "Iteration 70, Epoch 2, Loss: 4428185.0\n",
      "Iteration 80, Epoch 2, Loss: 4442463.5\n",
      "Iteration 90, Epoch 2, Loss: 4425146.0\n",
      "Iteration 100, Epoch 3, Loss: 4706651.5\n",
      "Iteration 110, Epoch 3, Loss: 4406599.5\n",
      "Iteration 120, Epoch 3, Loss: 4427624.0\n",
      "Iteration 130, Epoch 3, Loss: 4441759.0\n",
      "Iteration 140, Epoch 3, Loss: 4424533.5\n",
      "Iteration 150, Epoch 4, Loss: 4704949.5\n",
      "Iteration 160, Epoch 4, Loss: 4404856.5\n",
      "Iteration 170, Epoch 4, Loss: 4425956.0\n",
      "Iteration 180, Epoch 4, Loss: 4440194.0\n",
      "Iteration 190, Epoch 4, Loss: 4422988.5\n",
      "Iteration 200, Epoch 5, Loss: 4705428.0\n",
      "Iteration 210, Epoch 5, Loss: 4402840.0\n",
      "Iteration 220, Epoch 5, Loss: 4424338.5\n",
      "Iteration 230, Epoch 5, Loss: 4438575.5\n",
      "Iteration 240, Epoch 5, Loss: 4421324.5\n",
      "Iteration 250, Epoch 6, Loss: 4704075.5\n",
      "Iteration 260, Epoch 6, Loss: 4402398.0\n",
      "Iteration 270, Epoch 6, Loss: 4423549.5\n",
      "Iteration 280, Epoch 6, Loss: 4437516.5\n",
      "Iteration 290, Epoch 6, Loss: 4420292.5\n",
      "Iteration 300, Epoch 7, Loss: 4699650.5\n",
      "Iteration 310, Epoch 7, Loss: 4401503.5\n",
      "Iteration 320, Epoch 7, Loss: 4422696.0\n",
      "Iteration 330, Epoch 7, Loss: 4437004.5\n",
      "Iteration 340, Epoch 7, Loss: 4419693.5\n",
      "Iteration 350, Epoch 8, Loss: 4700834.5\n",
      "Iteration 360, Epoch 8, Loss: 4399529.0\n",
      "Iteration 370, Epoch 8, Loss: 4420367.0\n",
      "Iteration 380, Epoch 8, Loss: 4434918.0\n",
      "Iteration 390, Epoch 8, Loss: 4418058.5\n",
      "Iteration 400, Epoch 9, Loss: 4699184.0\n",
      "Iteration 410, Epoch 9, Loss: 4398184.0\n",
      "Iteration 420, Epoch 9, Loss: 4419359.0\n",
      "Iteration 430, Epoch 9, Loss: 4433872.0\n",
      "Iteration 440, Epoch 9, Loss: 4416845.5\n",
      "Iteration 450, Epoch 10, Loss: 4697110.0\n",
      "Iteration 460, Epoch 10, Loss: 4397051.5\n",
      "Iteration 470, Epoch 10, Loss: 4418432.5\n",
      "Iteration 480, Epoch 10, Loss: 4432822.0\n",
      "Iteration 490, Epoch 10, Loss: 4415872.5\n"
     ]
    }
   ],
   "source": [
    "Train(model, optimizer=optimizer, x=X_clean_train, num_epochs=10, batch_size = batch_size, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1292
    },
    "colab_type": "code",
    "id": "7rHKTz4SWxZa",
    "outputId": "b3fca03d-facc-45dd-af58-a870f232005e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Epoch 1, Loss: 4697745.5\n",
      "Iteration 10, Epoch 1, Loss: 4395869.0\n",
      "Iteration 20, Epoch 1, Loss: 4416639.0\n",
      "Iteration 30, Epoch 1, Loss: 4430612.0\n",
      "Iteration 40, Epoch 1, Loss: 4413318.0\n",
      "Iteration 50, Epoch 2, Loss: 4690193.5\n",
      "Iteration 60, Epoch 2, Loss: 4391524.5\n",
      "Iteration 70, Epoch 2, Loss: 4413672.5\n",
      "Iteration 80, Epoch 2, Loss: 4428306.0\n",
      "Iteration 90, Epoch 2, Loss: 4411500.0\n",
      "Iteration 100, Epoch 3, Loss: 4693664.5\n",
      "Iteration 110, Epoch 3, Loss: 4391505.5\n",
      "Iteration 120, Epoch 3, Loss: 4413021.0\n",
      "Iteration 130, Epoch 3, Loss: 4427709.0\n",
      "Iteration 140, Epoch 3, Loss: 4410805.5\n",
      "Iteration 150, Epoch 4, Loss: 4689900.0\n",
      "Iteration 160, Epoch 4, Loss: 4392024.5\n",
      "Iteration 170, Epoch 4, Loss: 4413300.0\n",
      "Iteration 180, Epoch 4, Loss: 4427620.5\n",
      "Iteration 190, Epoch 4, Loss: 4410627.0\n",
      "Iteration 200, Epoch 5, Loss: 4691763.0\n",
      "Iteration 210, Epoch 5, Loss: 4391096.0\n",
      "Iteration 220, Epoch 5, Loss: 4412045.5\n",
      "Iteration 230, Epoch 5, Loss: 4426632.5\n",
      "Iteration 240, Epoch 5, Loss: 4409722.0\n",
      "Iteration 250, Epoch 6, Loss: 4689225.5\n",
      "Iteration 260, Epoch 6, Loss: 4389470.0\n",
      "Iteration 270, Epoch 6, Loss: 4411552.5\n",
      "Iteration 280, Epoch 6, Loss: 4426140.0\n",
      "Iteration 290, Epoch 6, Loss: 4409359.0\n",
      "Iteration 300, Epoch 7, Loss: 4689393.0\n",
      "Iteration 310, Epoch 7, Loss: 4390407.5\n",
      "Iteration 320, Epoch 7, Loss: 4411613.0\n",
      "Iteration 330, Epoch 7, Loss: 4426080.5\n",
      "Iteration 340, Epoch 7, Loss: 4409007.0\n",
      "Iteration 350, Epoch 8, Loss: 4689557.0\n",
      "Iteration 360, Epoch 8, Loss: 4389042.0\n",
      "Iteration 370, Epoch 8, Loss: 4410479.0\n",
      "Iteration 380, Epoch 8, Loss: 4424942.5\n",
      "Iteration 390, Epoch 8, Loss: 4407988.5\n",
      "Iteration 400, Epoch 9, Loss: 4691098.0\n",
      "Iteration 410, Epoch 9, Loss: 4388800.0\n",
      "Iteration 420, Epoch 9, Loss: 4409996.0\n",
      "Iteration 430, Epoch 9, Loss: 4424464.0\n",
      "Iteration 440, Epoch 9, Loss: 4407387.5\n",
      "Iteration 450, Epoch 10, Loss: 4687649.5\n",
      "Iteration 460, Epoch 10, Loss: 4387464.0\n",
      "Iteration 470, Epoch 10, Loss: 4409431.0\n",
      "Iteration 480, Epoch 10, Loss: 4424017.5\n",
      "Iteration 490, Epoch 10, Loss: 4407047.5\n",
      "Iteration 500, Epoch 11, Loss: 4690437.0\n",
      "Iteration 510, Epoch 11, Loss: 4388184.5\n",
      "Iteration 520, Epoch 11, Loss: 4408920.5\n",
      "Iteration 530, Epoch 11, Loss: 4423508.0\n",
      "Iteration 540, Epoch 11, Loss: 4406511.5\n",
      "Iteration 550, Epoch 12, Loss: 4688061.0\n",
      "Iteration 560, Epoch 12, Loss: 4387927.5\n",
      "Iteration 570, Epoch 12, Loss: 4408717.0\n",
      "Iteration 580, Epoch 12, Loss: 4423221.5\n",
      "Iteration 590, Epoch 12, Loss: 4406059.5\n",
      "Iteration 600, Epoch 13, Loss: 4688768.5\n",
      "Iteration 610, Epoch 13, Loss: 4387394.0\n",
      "Iteration 620, Epoch 13, Loss: 4408002.0\n",
      "Iteration 630, Epoch 13, Loss: 4422374.5\n",
      "Iteration 640, Epoch 13, Loss: 4405352.0\n",
      "Iteration 650, Epoch 14, Loss: 4687211.5\n",
      "Iteration 660, Epoch 14, Loss: 4385815.5\n",
      "Iteration 670, Epoch 14, Loss: 4407276.0\n",
      "Iteration 680, Epoch 14, Loss: 4421826.0\n",
      "Iteration 690, Epoch 14, Loss: 4404732.0\n",
      "Iteration 700, Epoch 15, Loss: 4684817.5\n",
      "Iteration 710, Epoch 15, Loss: 4385237.0\n",
      "Iteration 720, Epoch 15, Loss: 4406564.0\n",
      "Iteration 730, Epoch 15, Loss: 4421417.5\n",
      "Iteration 740, Epoch 15, Loss: 4404543.5\n"
     ]
    }
   ],
   "source": [
    "Train(model, optimizer=optimizer, x=X_clean_train, num_epochs=15, batch_size = batch_size, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s4489NfZ58wm"
   },
   "outputs": [],
   "source": [
    "# model.load_weights('vae.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vkk7DRLDSHMU"
   },
   "outputs": [],
   "source": [
    "x_reco, mean, sigma  = model(X_clean_test[0:batch_size]/255, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "GDIUvSVKSHwv",
    "outputId": "d166e97f-d05a-4e0d-d94f-32b49495d3a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efc83e92dd8>"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrKWd20IikQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_clean_test[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "vCIfXG6CSH4a",
    "outputId": "d7d6033b-0cb0-4705-d374-f14ba938235e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efc83e77588>"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkSj/DNW5N1sw+/vCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.clip(x_reco[13]*255, 0, 255).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2BJxFkFJUrRY"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "XXqBN6VoM5tE",
    "outputId": "3b1165dd-5bc0-49f4-b51e-5d1c11b6556a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae_conv_net\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "e-conv1 (Conv2D)             multiple                  39        \n",
      "_________________________________________________________________\n",
      "e-conv2 (Conv2D)             multiple                  416       \n",
      "_________________________________________________________________\n",
      "e-conv3 (Conv2D)             multiple                  4128      \n",
      "_________________________________________________________________\n",
      "e-conv4 (Conv2D)             multiple                  4128      \n",
      "_________________________________________________________________\n",
      "e-sdnse (Dense)              multiple                  1048704   \n",
      "_________________________________________________________________\n",
      "e-mean (Dense)               multiple                  2064      \n",
      "_________________________________________________________________\n",
      "e-sigma (Dense)              multiple                  2064      \n",
      "_________________________________________________________________\n",
      "d-dense (Dense)              multiple                  2176      \n",
      "_________________________________________________________________\n",
      "d-dense2 (Dense)             multiple                  1056768   \n",
      "_________________________________________________________________\n",
      "d-convT1 (Conv2DTranspose)   multiple                  4128      \n",
      "_________________________________________________________________\n",
      "d-convT2 (Conv2DTranspose)   multiple                  4128      \n",
      "_________________________________________________________________\n",
      "d-convT3 (Conv2DTranspose)   multiple                  387       \n",
      "=================================================================\n",
      "Total params: 2,129,130\n",
      "Trainable params: 2,129,130\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "goGQAxqecYKU",
    "outputId": "9ca508dd-ca20-485b-e02a-46c7e117fff8"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# 1. Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "# 2. Save Keras Model or weights on google drive\n",
    "\n",
    "# create on Colab directory\n",
    "model.save_weights('vae.h5')    \n",
    "model_file = drive.CreateFile({'title' : 'vae.h5'})\n",
    "model_file.SetContentFile('vae.h5')\n",
    "model_file.Upload()\n",
    "\n",
    "# download to google drive\n",
    "drive.CreateFile({'id': model_file.get('id')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VaeDetectorNet(tf.keras.Model):\n",
    "    def __init__(self, input_shape = (32,32,3)):\n",
    "        super(VaeDecodervNet, self).__init__() \n",
    "        self.dense1 = tf.keras.layers.Dense(256, name=\"d-dense1\", activation=tf.nn.leaky_relu,kernel_initializer=initializer)\n",
    "        self.dense2 = tf.keras.layers.Dense(64, name=\"d-dense2\", activation=tf.nn.leaky_relu,kernel_initializer=initializer)\n",
    "        self.sigmoid = tf.keras.layers.Dense(1, name=\"d-sigmoid\", activation='sigmoid',kernel_initializer=tf.initializers.GlorotNormal())\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return self.sigmoid(x)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ed4eAp9aM5um"
   },
   "outputs": [],
   "source": [
    "# CNN based detector model\n",
    "def encode_model_init_fn():\n",
    "    input_shape = (32, 32, 3)\n",
    "    channel_1, channel_2, channel_3, channel_4, num_classes = 32, 64, 96, 128, 10\n",
    "    BN_decay        = 0.99\n",
    "    BN_epsilon      = 1.e-5\n",
    "    L2_weight_decay = 2e-5\n",
    "    droput_rate     = 0.11\n",
    "    \n",
    "    initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "    layers = [\n",
    "        tf.keras.layers.Conv2D(input_shape=input_shape, kernel_size=(3,3), filters=channel_1, activation='relu', padding='same', kernel_initializer=initializer, \n",
    "                             kernel_regularizer=tf.keras.regularizers.l2(L2_weight_decay),\n",
    "                             bias_regularizer=tf.keras.regularizers.l2(L2_weight_decay)),\n",
    "        tf.keras.layers.BatchNormalization(axis=-1, momentum=BN_decay, epsilon=BN_epsilon),\n",
    "        #tf.keras.layers.Dropout(droput_rate),\n",
    "        tf.keras.layers.Conv2D(kernel_size=(3,3), filters=channel_2, activation='relu', padding='same', kernel_initializer=initializer,\n",
    "                             kernel_regularizer=tf.keras.regularizers.l2(L2_weight_decay),\n",
    "                             bias_regularizer=tf.keras.regularizers.l2(L2_weight_decay)),\n",
    "        tf.keras.layers.BatchNormalization(axis=-1, momentum=BN_decay, epsilon=BN_epsilon),\n",
    "        tf.keras.layers.Dropout(droput_rate),\n",
    "        tf.keras.layers.Conv2D(kernel_size=(3,3), filters=channel_3, activation='relu', padding='same', kernel_initializer=initializer,\n",
    "                             kernel_regularizer=tf.keras.regularizers.l2(L2_weight_decay),\n",
    "                             bias_regularizer=tf.keras.regularizers.l2(L2_weight_decay)),\n",
    "        tf.keras.layers.BatchNormalization(axis=-1, momentum=BN_decay, epsilon=BN_epsilon),\n",
    "        tf.keras.layers.Dropout(droput_rate),\n",
    "        tf.keras.layers.Conv2D(kernel_size=(3,3), filters=channel_4, activation='relu', padding='same', kernel_initializer=initializer,\n",
    "                             kernel_regularizer=tf.keras.regularizers.l2(L2_weight_decay),\n",
    "                             bias_regularizer=tf.keras.regularizers.l2(L2_weight_decay)),\n",
    "        tf.keras.layers.GlobalMaxPooling2D(),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=initializer),\n",
    "    ]\n",
    "    model = tf.keras.Sequential(layers)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0PEXRuo8M5uq"
   },
   "outputs": [],
   "source": [
    "learning_rate = 5e-3\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum = 0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOdPcIm9M5uu"
   },
   "outputs": [],
   "source": [
    "model = model_init_fn()\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.binary_crossentropy,\n",
    "              metrics=[tf.keras.metrics.binary_accuracy])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nf4iLO3XM5ux",
    "outputId": "6298c135-4d27-4fca-963e-74b7d4bf172a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([47460    32    32     3], shape=(4,), dtype=int32)\n",
      "tf.Tensor([8256   32   32    3], shape=(4,), dtype=int32)\n",
      "tf.Tensor([55716    32    32     3], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.shape(X_clean_train_correct))\n",
    "print(tf.shape(X_clean_test_correct))\n",
    "X_d_trian = tf.concat( [X_clean_train_correct, X_clean_test_correct], axis = 0)\n",
    "print(tf.shape(X_d_trian))\n",
    "#Y_d_trian = tf.concat(Y_clean_train_correct, Y_clean_test_correct)\n",
    "Y_d_train = tf.ones(tf.shape(X_d_trian)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j3wuItDNM5u5",
    "outputId": "93997e37-5b41-499d-85ee-a250367b3f15"
   },
   "outputs": [],
   "source": [
    "model.fit(X_d_trian, Y_d_train, batch_size=64, epochs=1) #, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cb53MZEhM5vI"
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qC6MntyMM5vQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-oyeI6-dM5vo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "vae_detector_ldim_2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "TensorFlow-2.0",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
